@inproceedings{bhola-etal-2020-retrieving,
    title = "Retrieving Skills from Job Descriptions: A Language Model Based Extreme Multi-label Classification Framework",
    author = "Bhola, Akshay  and
      Halder, Kishaloy  and
      Prasad, Animesh  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.513",
    doi = "10.18653/v1/2020.coling-main.513",
    pages = "5832--5842",
    abstract = "We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65{\%} of job descriptions miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9{\%} and 7{\%} absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our model publicly available.",
}

@article{ Primary-Organizational-Recruitment-Sources,
author = {Shenoy, Varun and Aithal, Sreeramana},
year = {2018},
month = {04},
pages = {37-58},
title = {Literature Review on Primary Organizational Recruitment Sources},
journal = {International Journal of Management, Technology, and Social Sciences},
doi = {10.47992/IJMTS.2581.6012.0035}
}

@inproceedings{Self-trained-semi-supervised-learning,
author = {Hassanzadeh, Hamed and Kholghi, Mahnoosh and Nguyen, Anthony and Chu, Kevin},
year = {2018},
month = {11},
pages = {},
title = {Clinical Document Classification Using Labeled and Unlabeled Data Across Hospitals}
}

@inproceedings{knn-Zhang2019/08,
  title={Study of Employment Salary Forecast using KNN Algorithm},
  author={Junyu Zhang and Jinyong Cheng},
  year={2019/08},
  booktitle={Proceedings of the 2019 International Conference on Modeling, Simulation and Big Data Analysis (MSBDA 2019)},
  pages={166-170},
  issn={2352-538X},
  isbn={978-94-6252-784-3},
  url={https://doi.org/10.2991/msbda-19.2019.26},
  doi={10.2991/msbda-19.2019.26},
  publisher={Atlantis Press}
}

@book{manning2008introduction,
  title={Introduction to Information Retrieval},
  author={Manning, C.D. and Raghavan, P. and Sch{\"u}tze, H.},
  isbn={9780521865715},
  lccn={2008001257},
  series={An Introduction to Information Retrieval},
  url={https://books.google.com.au/books?id=GNvtngEACAAJ},
  year={2008},
  publisher={Cambridge University Press}
}

@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@InProceedings{10.1007/978-3-540-39964-3_62,
author="Guo, Gongde
and Wang, Hui
and Bell, David
and Bi, Yaxin
and Greer, Kieran",
editor="Meersman, Robert
and Tari, Zahir
and Schmidt, Douglas C.",
title="KNN Model-Based Approach in Classification",
booktitle="On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="986--996",
abstract="The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency -- being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a ``good value'' for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.",
isbn="978-3-540-39964-3"
}


@misc{liashchynskyi2019grid,
      title={Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS}, 
      author={Petro Liashchynskyi and Pavlo Liashchynskyi},
      year={2019},
      eprint={1912.06059},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{TAN2006290,
  title = {An effective refinement strategy for KNN text classifier},
  journal = {Expert Systems with Applications},
  volume = {30},
  number = {2},
  pages = {290-298},
  year = {2006},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2005.07.019},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417405001570},
  author = {Songbo Tan},
  keywords = {KNN, Text classification, Information retrieval, Data mining},
  abstract = {Due to the exponential growth of documents on the Internet and the emergent need to organize them, the automated categorization of documents into predefined labels has received an ever-increased attention in the recent years. A wide range of supervised learning algorithms has been introduced to deal with text classification. Among all these classifiers, K-Nearest Neighbors (KNN) is a widely used classifier in text categorization community because of its simplicity and efficiency. However, KNN still suffers from inductive biases or model misfits that result from its assumptions, such as the presumption that training data are evenly distributed among all categories. In this paper, we propose a new refinement strategy, which we called as DragPushing, for the KNN Classifier. The experiments on three benchmark evaluation collections show that DragPushing achieved a significant improvement on the performance of the KNN Classifier.}
}

@INPROCEEDINGS{dt_8718711,
  author={Dutta, Sananda and Halder, Airiddha and Dasgupta, Kousik},
  booktitle={2018 Fourth International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)}, 
  title={Design of a novel Prediction Engine for predicting suitable salary for a job}, 
  year={2018},
  volume={},
  number={},
  pages={275-279},
  doi={10.1109/ICRCICN.2018.8718711}
}



@InProceedings{nb_10.1007/BFb0029444,
author="Mani, Subramani
and Pazzani, Michael J.
and West, John",
editor="Keravnou, Elpida
and Garbay, Catherine
and Baud, Robert
and Wyatt, Jeremy",
title="Knowledge discovery from a breast cancer database",
booktitle="Artificial Intelligence in Medicine",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="130--133",
abstract="We report on the use of various Machine Learning algorithms on an electronic database of breast cancer patients. The task was to predict breast cancer recurrence using a short subset of clinical attributes such as tumor presence, tumor size, invasive nature of tumor, number of lymph nodes involved, severity of lymphedema and stage of tumor. The predictive accuracy over fifty runs employing test sets not used to build the model were 63.4{\%}(Cart), 63.9{\%}(C45), 62.5{\%}(C45rules), 66.4{\%}(FOCL) and 68.3{\%}(Naive Bayes). An extension of the model using additional features and larger datasets is contemplated.",
isbn="978-3-540-68448-0"
}


@inproceedings{han-etal-2022-systematic,
    title = "Systematic Evaluation of Predictive Fairness",
    author = "Han, Xudong  and
      Shen, Aili  and
      Cohn, Trevor  and
      Baldwin, Timothy  and
      Frermann, Lea",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.6",
    pages = "68--81",
    abstract = "Mitigating bias in training on biased datasets is an important open problem. Several techniques have been proposed, however the typical evaluation regime is very limited, considering very narrow data conditions. For instance, the effect of target class imbalance and stereotyping is under-studied. To address this gap, we examine the performance of various debiasing methods across multiple tasks, spanning binary classification (Twitter sentiment), multi-class classification (profession prediction), and regression (valence prediction). Through extensive experimentation, we find that data conditions have a strong influence on relative model performance, and that general conclusions cannot be drawn about method efficacy when evaluating only on standard datasets, as is current practice in fairness research.",
}

@misc{svm_rejani2009early,
      title={Early Detection of Breast Cancer using SVM Classifier Technique}, 
      author={Y. Ireaneus Anna Rejani and S. Thamarai Selvi},
      year={2009},
      eprint={0912.2314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-1910-01842,
  author       = {Duc Tam Nguyen and
                  Chaithanya Kumar Mummadi and
                  Thi{-}Phuong{-}Nhung Ngo and
                  Thi Hoai Phuong Nguyen and
                  Laura Beggel and
                  Thomas Brox},
  title        = {{SELF:} Learning to Filter Noisy Labels with Self-Ensembling},
  journal      = {CoRR},
  volume       = {abs/1910.01842},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.01842},
  eprinttype    = {arXiv},
  eprint       = {1910.01842},
  timestamp    = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01842.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ning2021review,
  title={A review of research on co-training},
  author={Ning, Xin and Wang, Xinran and Xu, Shaohui and Cai, Weiwei and Zhang, Liping and Yu, Lina and Li, Wenfa},
  journal={Concurrency and computation: practice and experience},
  pages={e6276},
  year={2021},
  publisher={Wiley Online Library}
}

@ARTICLE{4359365,
  author={Culp, Mark and Michailidis, George},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Graph-Based Semisupervised Learning}, 
  year={2008},
  volume={30},
  number={1},
  pages={174-179},
  doi={10.1109/TPAMI.2007.70765}}
