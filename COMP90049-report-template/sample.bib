@inproceedings{bhola-etal-2020-retrieving,
    title = "Retrieving Skills from Job Descriptions: A Language Model Based Extreme Multi-label Classification Framework",
    author = "Bhola, Akshay  and
      Halder, Kishaloy  and
      Prasad, Animesh  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.513",
    doi = "10.18653/v1/2020.coling-main.513",
    pages = "5832--5842",
    abstract = "We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65{\%} of job descriptions miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9{\%} and 7{\%} absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our model publicly available.",
}

@article{ Primary-Organizational-Recruitment-Sources,
author = {Shenoy, Varun and Aithal, Sreeramana},
year = {2018},
month = {04},
pages = {37-58},
title = {Literature Review on Primary Organizational Recruitment Sources},
journal = {International Journal of Management, Technology, and Social Sciences},
doi = {10.47992/IJMTS.2581.6012.0035}
}

@inproceedings{Self-trained-semi-supervised-learning,
author = {Hassanzadeh, Hamed and Kholghi, Mahnoosh and Nguyen, Anthony and Chu, Kevin},
year = {2018},
month = {11},
pages = {},
title = {Clinical Document Classification Using Labeled and Unlabeled Data Across Hospitals}
}

@inproceedings{knn-Zhang2019/08,
  title={Study of Employment Salary Forecast using KNN Algorithm},
  author={Junyu Zhang and Jinyong Cheng},
  year={2019/08},
  booktitle={Proceedings of the 2019 International Conference on Modeling, Simulation and Big Data Analysis (MSBDA 2019)},
  pages={166-170},
  issn={2352-538X},
  isbn={978-94-6252-784-3},
  url={https://doi.org/10.2991/msbda-19.2019.26},
  doi={10.2991/msbda-19.2019.26},
  publisher={Atlantis Press}
}

@book{manning2008introduction,
  title={Introduction to Information Retrieval},
  author={Manning, C.D. and Raghavan, P. and Sch{\"u}tze, H.},
  isbn={9780521865715},
  lccn={2008001257},
  series={An Introduction to Information Retrieval},
  url={https://books.google.com.au/books?id=GNvtngEACAAJ},
  year={2008},
  publisher={Cambridge University Press}
}

@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}
